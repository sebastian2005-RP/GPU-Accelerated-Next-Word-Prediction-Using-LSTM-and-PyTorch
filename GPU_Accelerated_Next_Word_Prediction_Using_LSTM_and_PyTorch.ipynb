{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This script installs the Natural Language Toolkit (NLTK), a Python library used for natural language processing tasks such as tokenization, tagging, and text prediction."
      ],
      "metadata": {
        "id": "PMAA9n7QiPWE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk5TXlIvyGfu",
        "outputId": "f33c1d06-4dd1-4151-87bf-2aa1e5b90314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of script imports necessary libraries for deep learning with PyTorch.\n",
        "\n",
        "Sets up text preprocessing with NLTK, including tokenization and stopword removal."
      ],
      "metadata": {
        "id": "9sriRMyiiYAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0B-6nMxyghb",
        "outputId": "8e80523c-23da-4f52-ce37-1c2dc68ca4bd"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used the following dataset for training."
      ],
      "metadata": {
        "id": "F6KbvMUPix8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document=\"\"\"Exploring Data Availability in LLM Development\n",
        "When developing a large language model (LLM), it's crucial to consider the availability of\n",
        "labeled data for the specific task you want the model to perform. An LLM is a complex AI\n",
        "model trained to understand and generate human-like language based on patterns learned\n",
        "from vast amounts of text data. However, general-purpose LLMs often need fine-tuning—\n",
        " additional, focused training on a smaller, task-specific dataset—to perform well on a\n",
        "specialized task, like summarizing scientific articles or answering customer support queries.\n",
        "Fine-tuning, or adapting a model to a new task, is particularly valuable when data is limited.\n",
        "In such cases, fine-tuning the model with smaller, targeted datasets allows it to perform\n",
        "specialized tasks effectively. When labeled data is minimal, methods like zero-shot, few-shot,\n",
        "and multi-shot learning—referred to collectively as N-shot learning—become essential to\n",
        "adapt the model.\n",
        "Understanding Transfer Learning\n",
        "Transfer learning is a powerful AI approach that enables models trained on one task to apply\n",
        "the knowledge they gained to a related but different task. This is similar to how humans can\n",
        "transfer knowledge across skills. For example, a musician trained in piano can transfer skills\n",
        "like reading music and understanding rhythm to learning the guitar. In the context of LLMs, a\n",
        "model trained on broad, general text (like news articles, books, and websites) can transfer its\n",
        "language understanding to tasks that require specialized knowledge, like medical or legal\n",
        "language processing. Transfer learning lets the model reuse its base knowledge of language\n",
        "and semantics to perform well on a task for which it may not have specific training data.\n",
        "Zero-shot Learning\n",
        "Zero-shot learning is a technique that allows LLMs to perform tasks they haven’t explicitly\n",
        "trained for. It works by leveraging the model's broad understanding of language and context\n",
        "to apply this knowledge to new scenarios. Imagine a child who has never seen a zebra but\n",
        "knows what a horse looks like. If someone tells the child that a zebra looks like a \"striped\n",
        "horse,\" the child can identify the zebra without any specific training. Similarly, an LLM\n",
        "trained on a variety of text can use zero-shot learning to answer questions about topics it\n",
        "hasn’t directly learned by making educated guesses based on its general language\n",
        "understanding.\n",
        "Example:\n",
        "Suppose an LLM is asked to translate a sentence into a language it hasn’t been trained on\n",
        "directly. If the model has learned similarities and patterns in other languages, it might\n",
        "approximate the translation with some level of accuracy, even without having any data on\n",
        "that specific language.\n",
        "Few-shot Learning\n",
        "Few-shot learning allows a model to learn a new task with only a few examples. This\n",
        "approach relies on the model’s ability to generalize from previous tasks, making it more\n",
        "adaptable to new ones even with limited examples. For instance, a student who has attended\n",
        "lectures on a topic might answer an exam question based on what they learned in class\n",
        "without much additional study. Few-shot learning similarly enables LLMs to perform a new\n",
        "task effectively with just a small number of training examples.\n",
        "Example:\n",
        "If an LLM has been trained to understand language structure and is given only three or four\n",
        "labeled examples of how to summarize news articles, it can still generalize well enough to\n",
        "summarize new articles by using those few examples to infer the general rules of\n",
        "summarization.\n",
        "One-shot Learning as Part of Few-shot Learning\n",
        "A specific case of few-shot learning, one-shot learning, requires only a single example to\n",
        "teach the model a task. For example, suppose a student sees one example of how to solve a\n",
        "math problem. They might then apply that single example to solve similar problems on their\n",
        "own. For LLMs, one-shot learning is useful when training data is particularly scarce but the\n",
        "model can generalize well enough from just one labeled example.\n",
        "Example:\n",
        "If you want the model to recognize a new product category (like \"smart thermostats\") and\n",
        "you provide only one example of a product description in this category, the model may use\n",
        "that single instance to identify other smart thermostat products based on similarities in\n",
        "language and function.\n",
        "Multi-shot Learning\n",
        "Multi-shot learning is similar to few-shot learning but involves more examples, which\n",
        "typically improves the model's accuracy and generalization. This approach requires a set of\n",
        "labeled examples for the model to learn from, though it’s still smaller than the amount\n",
        "required for traditional supervised learning. Multi-shot learning strikes a balance between\n",
        "extensive training data and the adaptability of fewer examples.\n",
        "Example:\n",
        "Imagine training an LLM to recognize different dog breeds. By showing it several images of\n",
        "a Golden Retriever, the model starts learning the features of this breed. With a few more\n",
        "images of similar breeds, like Labradors, it can generalize its knowledge to recognize these as\n",
        "well, enabling it to distinguish breeds without needing thousands of examples.\n",
        "Task:\n",
        "Question: You are part of a team working on an innovative project aiming to adapt a pre\n",
        "trained language model to a new, related task without much data. To ensure the project's\n",
        "success, you need to adapt and fine-tune the model. Which general approach leverages prior\n",
        "knowledge from one task to help train a model on a new, related task?\n",
        "Select one answer:\n",
        "1. N-shot learning\n",
        "2. Zero-shot learning\n",
        "3. Few-shot learning\n",
        "4. Transfer learning\n",
        "5. One-shot learning\n",
        "The correct answer is:\n",
        "4. Transfer learning\n",
        "Building Blocks to Train LLMs\n",
        "In this section we focuses on two core techniques to pre-train large language models (LLMs)\n",
        "— next word prediction and masked language modeling. These methods serve as\n",
        "foundational steps in training many advanced language models, including those used in\n",
        "natural language processing (NLP) tasks. Pre-training a model involves using a massive\n",
        "dataset to give the model a general understanding of language before it’s fine-tuned for\n",
        "specific tasks. Although pre-training from scratch can be costly and time-consuming, many\n",
        "organizations fine-tune pre-existing pre-trained models instead, adapting them to their\n",
        "particular needs.\n",
        "Generative Pre-Training\n",
        "Generative pre-training is a technique where the model is given sequences of words or text\n",
        "tokens and learns to predict the next token in that sequence. Through repeated exposure to\n",
        "different text sequences, the model learns to generate language that is coherent and\n",
        "contextually relevant. This pre-training process lays the groundwork for the model’s ability to\n",
        "understand and produce natural language. Two main types of generative pre-training\n",
        "techniques are next word prediction and masked language modeling, both of which allow the\n",
        "model to learn patterns, relationships, and the contextual meaning of words.\n",
        "1. Next Word Prediction\n",
        "Next word prediction is a supervised learning technique where the model is trained to predict\n",
        "the next word in a sequence based on the words that come before it. In supervised learning,\n",
        "the model learns from labeled data—in this case, sentences with a specific sequence of\n",
        "words. As the model processes each word in a sentence, it builds a contextual understanding\n",
        "of how words typically follow one another.\n",
        "For example, in the sentence “The quick brown fox jumps over the lazy dog,” the model\n",
        "might be given the input “The quick brown” and be trained to predict the word “fox” as the\n",
        "most likely next word. After correctly predicting “fox,” this word is added to the input\n",
        "sequence, creating “The quick brown fox,” and the model then tries to predict “jumps.” This\n",
        "process continues, with each prediction added to the sequence, helping the model capture\n",
        "dependencies between words and improve at generating coherent text. Suppose you give the\n",
        "model a prompt, like \"I like to drink coffee in the __.\" The model, having seen many similar\n",
        "sentences during training, will likely predict \"morning\" as the next word based on the\n",
        "common association between coffee and morning routines.\n",
        "Training Data for Next Word Prediction\n",
        "To train the model, large datasets are used to create numerous input-output pairs. Each output\n",
        "is then added back into the sequence for the next input, helping the model learn longer\n",
        "patterns and more complex word dependencies. Using a single sentence, like “The quick\n",
        "brown fox jumps over the lazy dog,” training pairs might look like this:\n",
        " Input: “The quick brown” → Output: “fox”\n",
        " Input: “The quick brown fox” → Output: “jumps”\n",
        " Input: “The quick brown fox jumps” → Output: “over”\n",
        "Through many such examples, the model begins to understand common word associations.\n",
        "For instance, when prompted with “I like to eat pizza with __,” it might predict “cheese”\n",
        "rather than words like “oregano” or “ketchup,” because it has learned that “cheese”\n",
        "frequently appears with “pizza” in similar contexts. This type of learning lets the model\n",
        "generate more accurate and realistic sentences.\n",
        "2. Masked Language Modeling\n",
        "Masked language modeling (MLM) is another popular technique for generative pre-training,\n",
        "but instead of predicting the next word in a sequence, it involves predicting a word that has\n",
        "been “masked” or hidden within a sentence. This approach challenges the model to infer\n",
        "missing information from surrounding words, helping it learn contextual clues and develop a\n",
        "nuanced understanding of language.\n",
        "In MLM, a word within a sentence is randomly replaced with a “[MASK]” token. For\n",
        "example, in the sentence “The quick brown fox jumps over the lazy dog,” the word “brown”\n",
        "might be masked, so the input becomes “The quick [MASK] fox jumps over the lazy dog.”\n",
        "The model is trained to predict the missing word (“brown”) by analyzing the context\n",
        "provided by the rest of the sentence. Even though “brown” could theoretically be replaced by\n",
        "many different adjectives, the model learns through training data that “brown” is the most\n",
        "likely option here. Suppose the model encounters the sentence “I enjoy reading books on\n",
        "[MASK] weekends.” Based on its prior training, the model will likely predict “the” as the\n",
        "masked word, since “on the weekends” is a common phrase structure. This ability to predict\n",
        "missing words based on context helps the model develop a better sense of language structure\n",
        "and word relationships.\n",
        "Task:\n",
        "Question: As part of a sales company's AI development team, you have been asked to\n",
        "explain how masked language modeling works to business stakeholders. You present a\n",
        "sample of masked data to help illustrate this pre-training process:\n",
        "Sample: \"The [MASK] support [MASK] quickly resolved the [MASK].\"\n",
        "What words have been masked?\n",
        "Possible Options:\n",
        "1. office, manager, fight\n",
        "2. work, dog, bone\n",
        "3. customer, agent, issue\n",
        "4. station, officer, feedback\n",
        "Correct Answer:\n",
        "3. customer, agent, issue\n",
        "Question:\n",
        "You have been working on training an LLM using next word prediction. You have provided\n",
        "the model with the following training data to help it learn how to predict the next word:\n",
        " What is\n",
        " What is the\n",
        " What is the weather\n",
        " What is the weather like\n",
        " What is the weather like ……\n",
        "Which would be the correct prediction for the next word(s)?\n",
        "Possible Options:\n",
        "1. \"in the cupboard?\"\n",
        "2. \"today?\"\n",
        "3. \"I don't know.\"\n",
        "4. \"rainy?\"\n",
        "Correct Answer:\n",
        "2. \"today?\"\n",
        "Question:\n",
        "You are a data scientist planning to develop large language models from scratch, which\n",
        "involves building a large generic model for different applications the organization anticipates.\n",
        "The organization also intends to build a customer service bot to address the high volume of\n",
        "customer queries. To ensure optimal performance of their AI-driven chatbot, you are\n",
        "expected to use a combination of techniques in a specific order.\n",
        "Arrange the techniques in the order the company should use them for their language\n",
        "model:\n",
        "1. Tokenize, remove stop words, and lemmatize the raw text\n",
        "2. Generate word embeddings to convert language to numbers\n",
        "3. Train the model using masked language modeling\n",
        "4. Fine-tune the model using task-specific data\n",
        "Answer:\n",
        "1. Tokenize, remove stop words, and lemmatize the raw text – Start by preprocessing the\n",
        "text data to ensure that the raw text is clean and standardized.\n",
        "2. Generate word embeddings to convert language to numbers – Convert the cleaned text\n",
        "into numerical representations that the model can process.\n",
        "3. Train the model using masked language modeling – Use masked language modeling\n",
        "to pre-train the model on a large dataset, allowing it to understand language structure\n",
        "and context.\n",
        "4. Fine-tune the model using task-specific data – Finally, adapt the pre-trained model to\n",
        "the specific customer service task by fine-tuning it on relevant labeled data. \"\"\""
      ],
      "metadata": {
        "id": "QJ4oxnZI0Ayi"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script prepares text data for a next-word prediction model using PyTorch, with tokenization and stopword removal handled via NLTK."
      ],
      "metadata": {
        "id": "WXH1afiGjExH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnrvkk_e03RD",
        "outputId": "435a2ec4-6cb2-4764-a59b-f9fdc5946cb4"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script preprocesses a text dataset for next-word prediction by tokenizing the input document and converting it to lowercase using NLTK."
      ],
      "metadata": {
        "id": "9e7Zhj3ljRue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(document.lower())"
      ],
      "metadata": {
        "id": "zoPcPe-e0Ut4"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8raDM1Uw0rJ1",
        "outputId": "d8fdd86d-2799-4cf3-d5a9-37161a98f86e"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['exploring',\n",
              " 'data',\n",
              " 'availability',\n",
              " 'in',\n",
              " 'llm',\n",
              " 'development',\n",
              " 'when',\n",
              " 'developing',\n",
              " 'a',\n",
              " 'large',\n",
              " 'language',\n",
              " 'model',\n",
              " '(',\n",
              " 'llm',\n",
              " ')',\n",
              " ',',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'crucial',\n",
              " 'to',\n",
              " 'consider',\n",
              " 'the',\n",
              " 'availability',\n",
              " 'of',\n",
              " 'labeled',\n",
              " 'data',\n",
              " 'for',\n",
              " 'the',\n",
              " 'specific',\n",
              " 'task',\n",
              " 'you',\n",
              " 'want',\n",
              " 'the',\n",
              " 'model',\n",
              " 'to',\n",
              " 'perform',\n",
              " '.',\n",
              " 'an',\n",
              " 'llm',\n",
              " 'is',\n",
              " 'a',\n",
              " 'complex',\n",
              " 'ai',\n",
              " 'model',\n",
              " 'trained',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'and',\n",
              " 'generate',\n",
              " 'human-like',\n",
              " 'language',\n",
              " 'based',\n",
              " 'on',\n",
              " 'patterns',\n",
              " 'learned',\n",
              " 'from',\n",
              " 'vast',\n",
              " 'amounts',\n",
              " 'of',\n",
              " 'text',\n",
              " 'data',\n",
              " '.',\n",
              " 'however',\n",
              " ',',\n",
              " 'general-purpose',\n",
              " 'llms',\n",
              " 'often',\n",
              " 'need',\n",
              " 'fine-tuning—',\n",
              " 'additional',\n",
              " ',',\n",
              " 'focused',\n",
              " 'training',\n",
              " 'on',\n",
              " 'a',\n",
              " 'smaller',\n",
              " ',',\n",
              " 'task-specific',\n",
              " 'dataset—to',\n",
              " 'perform',\n",
              " 'well',\n",
              " 'on',\n",
              " 'a',\n",
              " 'specialized',\n",
              " 'task',\n",
              " ',',\n",
              " 'like',\n",
              " 'summarizing',\n",
              " 'scientific',\n",
              " 'articles',\n",
              " 'or',\n",
              " 'answering',\n",
              " 'customer',\n",
              " 'support',\n",
              " 'queries',\n",
              " '.',\n",
              " 'fine-tuning',\n",
              " ',',\n",
              " 'or',\n",
              " 'adapting',\n",
              " 'a',\n",
              " 'model',\n",
              " 'to',\n",
              " 'a',\n",
              " 'new',\n",
              " 'task',\n",
              " ',',\n",
              " 'is',\n",
              " 'particularly',\n",
              " 'valuable',\n",
              " 'when',\n",
              " 'data',\n",
              " 'is',\n",
              " 'limited',\n",
              " '.',\n",
              " 'in',\n",
              " 'such',\n",
              " 'cases',\n",
              " ',',\n",
              " 'fine-tuning',\n",
              " 'the',\n",
              " 'model',\n",
              " 'with',\n",
              " 'smaller',\n",
              " ',',\n",
              " 'targeted',\n",
              " 'datasets',\n",
              " 'allows',\n",
              " 'it',\n",
              " 'to',\n",
              " 'perform',\n",
              " 'specialized',\n",
              " 'tasks',\n",
              " 'effectively',\n",
              " '.',\n",
              " 'when',\n",
              " 'labeled',\n",
              " 'data',\n",
              " 'is',\n",
              " 'minimal',\n",
              " ',',\n",
              " 'methods',\n",
              " 'like',\n",
              " 'zero-shot',\n",
              " ',',\n",
              " 'few-shot',\n",
              " ',',\n",
              " 'and',\n",
              " 'multi-shot',\n",
              " 'learning—referred',\n",
              " 'to',\n",
              " 'collectively',\n",
              " 'as',\n",
              " 'n-shot',\n",
              " 'learning—become',\n",
              " 'essential',\n",
              " 'to',\n",
              " 'adapt',\n",
              " 'the',\n",
              " 'model',\n",
              " '.',\n",
              " 'understanding',\n",
              " 'transfer',\n",
              " 'learning',\n",
              " 'transfer',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'a',\n",
              " 'powerful',\n",
              " 'ai',\n",
              " 'approach',\n",
              " 'that',\n",
              " 'enables',\n",
              " 'models',\n",
              " 'trained',\n",
              " 'on',\n",
              " 'one',\n",
              " 'task',\n",
              " 'to',\n",
              " 'apply',\n",
              " 'the',\n",
              " 'knowledge',\n",
              " 'they',\n",
              " 'gained',\n",
              " 'to',\n",
              " 'a',\n",
              " 'related',\n",
              " 'but',\n",
              " 'different',\n",
              " 'task',\n",
              " '.',\n",
              " 'this',\n",
              " 'is',\n",
              " 'similar',\n",
              " 'to',\n",
              " 'how',\n",
              " 'humans',\n",
              " 'can',\n",
              " 'transfer',\n",
              " 'knowledge',\n",
              " 'across',\n",
              " 'skills',\n",
              " '.',\n",
              " 'for',\n",
              " 'example',\n",
              " ',',\n",
              " 'a',\n",
              " 'musician',\n",
              " 'trained',\n",
              " 'in',\n",
              " 'piano',\n",
              " 'can',\n",
              " 'transfer',\n",
              " 'skills',\n",
              " 'like',\n",
              " 'reading',\n",
              " 'music',\n",
              " 'and',\n",
              " 'understanding',\n",
              " 'rhythm',\n",
              " 'to',\n",
              " 'learning',\n",
              " 'the',\n",
              " 'guitar',\n",
              " '.',\n",
              " 'in',\n",
              " 'the',\n",
              " 'context',\n",
              " 'of',\n",
              " 'llms',\n",
              " ',',\n",
              " 'a',\n",
              " 'model',\n",
              " 'trained',\n",
              " 'on',\n",
              " 'broad',\n",
              " ',',\n",
              " 'general',\n",
              " 'text',\n",
              " '(',\n",
              " 'like',\n",
              " 'news',\n",
              " 'articles',\n",
              " ',',\n",
              " 'books',\n",
              " ',',\n",
              " 'and',\n",
              " 'websites',\n",
              " ')',\n",
              " 'can',\n",
              " 'transfer',\n",
              " 'its',\n",
              " 'language',\n",
              " 'understanding',\n",
              " 'to',\n",
              " 'tasks',\n",
              " 'that',\n",
              " 'require',\n",
              " 'specialized',\n",
              " 'knowledge',\n",
              " ',',\n",
              " 'like',\n",
              " 'medical',\n",
              " 'or',\n",
              " 'legal',\n",
              " 'language',\n",
              " 'processing',\n",
              " '.',\n",
              " 'transfer',\n",
              " 'learning',\n",
              " 'lets',\n",
              " 'the',\n",
              " 'model',\n",
              " 'reuse',\n",
              " 'its',\n",
              " 'base',\n",
              " 'knowledge',\n",
              " 'of',\n",
              " 'language',\n",
              " 'and',\n",
              " 'semantics',\n",
              " 'to',\n",
              " 'perform',\n",
              " 'well',\n",
              " 'on',\n",
              " 'a',\n",
              " 'task',\n",
              " 'for',\n",
              " 'which',\n",
              " 'it',\n",
              " 'may',\n",
              " 'not',\n",
              " 'have',\n",
              " 'specific',\n",
              " 'training',\n",
              " 'data',\n",
              " '.',\n",
              " 'zero-shot',\n",
              " 'learning',\n",
              " 'zero-shot',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'a',\n",
              " 'technique',\n",
              " 'that',\n",
              " 'allows',\n",
              " 'llms',\n",
              " 'to',\n",
              " 'perform',\n",
              " 'tasks',\n",
              " 'they',\n",
              " 'haven',\n",
              " '’',\n",
              " 't',\n",
              " 'explicitly',\n",
              " 'trained',\n",
              " 'for',\n",
              " '.',\n",
              " 'it',\n",
              " 'works',\n",
              " 'by',\n",
              " 'leveraging',\n",
              " 'the',\n",
              " 'model',\n",
              " \"'s\",\n",
              " 'broad',\n",
              " 'understanding',\n",
              " 'of',\n",
              " 'language',\n",
              " 'and',\n",
              " 'context',\n",
              " 'to',\n",
              " 'apply',\n",
              " 'this',\n",
              " 'knowledge',\n",
              " 'to',\n",
              " 'new',\n",
              " 'scenarios',\n",
              " '.',\n",
              " 'imagine',\n",
              " 'a',\n",
              " 'child',\n",
              " 'who',\n",
              " 'has',\n",
              " 'never',\n",
              " 'seen',\n",
              " 'a',\n",
              " 'zebra',\n",
              " 'but',\n",
              " 'knows',\n",
              " 'what',\n",
              " 'a',\n",
              " 'horse',\n",
              " 'looks',\n",
              " 'like',\n",
              " '.',\n",
              " 'if',\n",
              " 'someone',\n",
              " 'tells',\n",
              " 'the',\n",
              " 'child',\n",
              " 'that',\n",
              " 'a',\n",
              " 'zebra',\n",
              " 'looks',\n",
              " 'like',\n",
              " 'a',\n",
              " '``',\n",
              " 'striped',\n",
              " 'horse',\n",
              " ',',\n",
              " \"''\",\n",
              " 'the',\n",
              " 'child',\n",
              " 'can',\n",
              " 'identify',\n",
              " 'the',\n",
              " 'zebra',\n",
              " 'without',\n",
              " 'any',\n",
              " 'specific',\n",
              " 'training',\n",
              " '.',\n",
              " 'similarly',\n",
              " ',',\n",
              " 'an',\n",
              " 'llm',\n",
              " 'trained',\n",
              " 'on',\n",
              " 'a',\n",
              " 'variety',\n",
              " 'of',\n",
              " 'text',\n",
              " 'can',\n",
              " 'use',\n",
              " 'zero-shot',\n",
              " 'learning',\n",
              " 'to',\n",
              " 'answer',\n",
              " 'questions',\n",
              " 'about',\n",
              " 'topics',\n",
              " 'it',\n",
              " 'hasn',\n",
              " '’',\n",
              " 't',\n",
              " 'directly',\n",
              " 'learned',\n",
              " 'by',\n",
              " 'making',\n",
              " 'educated',\n",
              " 'guesses',\n",
              " 'based',\n",
              " 'on',\n",
              " 'its',\n",
              " 'general',\n",
              " 'language',\n",
              " 'understanding',\n",
              " '.',\n",
              " 'example',\n",
              " ':',\n",
              " 'suppose',\n",
              " 'an',\n",
              " 'llm',\n",
              " 'is',\n",
              " 'asked',\n",
              " 'to',\n",
              " 'translate',\n",
              " 'a',\n",
              " 'sentence',\n",
              " 'into',\n",
              " 'a',\n",
              " 'language',\n",
              " 'it',\n",
              " 'hasn',\n",
              " '’',\n",
              " 't',\n",
              " 'been',\n",
              " 'trained',\n",
              " 'on',\n",
              " 'directly',\n",
              " '.',\n",
              " 'if',\n",
              " 'the',\n",
              " 'model',\n",
              " 'has',\n",
              " 'learned',\n",
              " 'similarities',\n",
              " 'and',\n",
              " 'patterns',\n",
              " 'in',\n",
              " 'other',\n",
              " 'languages',\n",
              " ',',\n",
              " 'it',\n",
              " 'might',\n",
              " 'approximate',\n",
              " 'the',\n",
              " 'translation',\n",
              " 'with',\n",
              " 'some',\n",
              " 'level',\n",
              " 'of',\n",
              " 'accuracy',\n",
              " ',',\n",
              " 'even',\n",
              " 'without',\n",
              " 'having',\n",
              " 'any',\n",
              " 'data',\n",
              " 'on',\n",
              " 'that',\n",
              " 'specific',\n",
              " 'language',\n",
              " '.',\n",
              " 'few-shot',\n",
              " 'learning',\n",
              " 'few-shot',\n",
              " 'learning',\n",
              " 'allows',\n",
              " 'a',\n",
              " 'model',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'a',\n",
              " 'new',\n",
              " 'task',\n",
              " 'with',\n",
              " 'only',\n",
              " 'a',\n",
              " 'few',\n",
              " 'examples',\n",
              " '.',\n",
              " 'this',\n",
              " 'approach',\n",
              " 'relies',\n",
              " 'on',\n",
              " 'the',\n",
              " 'model',\n",
              " '’',\n",
              " 's',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'generalize',\n",
              " 'from',\n",
              " 'previous',\n",
              " 'tasks',\n",
              " ',',\n",
              " 'making',\n",
              " 'it',\n",
              " 'more',\n",
              " 'adaptable',\n",
              " 'to',\n",
              " 'new',\n",
              " 'ones',\n",
              " 'even',\n",
              " 'with',\n",
              " 'limited',\n",
              " 'examples',\n",
              " '.',\n",
              " 'for',\n",
              " 'instance',\n",
              " ',',\n",
              " 'a',\n",
              " 'student',\n",
              " 'who',\n",
              " 'has',\n",
              " 'attended',\n",
              " 'lectures',\n",
              " 'on',\n",
              " 'a',\n",
              " 'topic',\n",
              " 'might',\n",
              " 'answer',\n",
              " 'an',\n",
              " 'exam',\n",
              " 'question',\n",
              " 'based',\n",
              " 'on',\n",
              " 'what',\n",
              " 'they',\n",
              " 'learned',\n",
              " 'in',\n",
              " 'class',\n",
              " 'without',\n",
              " 'much',\n",
              " 'additional',\n",
              " 'study',\n",
              " '.',\n",
              " 'few-shot',\n",
              " 'learning',\n",
              " 'similarly',\n",
              " 'enables',\n",
              " 'llms',\n",
              " 'to',\n",
              " 'perform',\n",
              " 'a',\n",
              " 'new',\n",
              " 'task',\n",
              " 'effectively',\n",
              " 'with',\n",
              " 'just',\n",
              " 'a',\n",
              " 'small',\n",
              " 'number',\n",
              " 'of',\n",
              " 'training',\n",
              " 'examples',\n",
              " '.',\n",
              " 'example',\n",
              " ':',\n",
              " 'if',\n",
              " 'an',\n",
              " 'llm',\n",
              " 'has',\n",
              " 'been',\n",
              " 'trained',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'language',\n",
              " 'structure',\n",
              " 'and',\n",
              " 'is',\n",
              " 'given',\n",
              " 'only',\n",
              " 'three',\n",
              " 'or',\n",
              " 'four',\n",
              " 'labeled',\n",
              " 'examples',\n",
              " 'of',\n",
              " 'how',\n",
              " 'to',\n",
              " 'summarize',\n",
              " 'news',\n",
              " 'articles',\n",
              " ',',\n",
              " 'it',\n",
              " 'can',\n",
              " 'still',\n",
              " 'generalize',\n",
              " 'well',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'summarize',\n",
              " 'new',\n",
              " 'articles',\n",
              " 'by',\n",
              " 'using',\n",
              " 'those',\n",
              " 'few',\n",
              " 'examples',\n",
              " 'to',\n",
              " 'infer',\n",
              " 'the',\n",
              " 'general',\n",
              " 'rules',\n",
              " 'of',\n",
              " 'summarization',\n",
              " '.',\n",
              " 'one-shot',\n",
              " 'learning',\n",
              " 'as',\n",
              " 'part',\n",
              " 'of',\n",
              " 'few-shot',\n",
              " 'learning',\n",
              " 'a',\n",
              " 'specific',\n",
              " 'case',\n",
              " 'of',\n",
              " 'few-shot',\n",
              " 'learning',\n",
              " ',',\n",
              " 'one-shot',\n",
              " 'learning',\n",
              " ',',\n",
              " 'requires',\n",
              " 'only',\n",
              " 'a',\n",
              " 'single',\n",
              " 'example',\n",
              " 'to',\n",
              " 'teach',\n",
              " 'the',\n",
              " 'model',\n",
              " 'a',\n",
              " 'task',\n",
              " '.',\n",
              " 'for',\n",
              " 'example',\n",
              " ',',\n",
              " 'suppose',\n",
              " 'a',\n",
              " 'student',\n",
              " 'sees',\n",
              " 'one',\n",
              " 'example',\n",
              " 'of',\n",
              " 'how',\n",
              " 'to',\n",
              " 'solve',\n",
              " 'a',\n",
              " 'math',\n",
              " 'problem',\n",
              " '.',\n",
              " 'they',\n",
              " 'might',\n",
              " 'then',\n",
              " 'apply',\n",
              " 'that',\n",
              " 'single',\n",
              " 'example',\n",
              " 'to',\n",
              " 'solve',\n",
              " 'similar',\n",
              " 'problems',\n",
              " 'on',\n",
              " 'their',\n",
              " 'own',\n",
              " '.',\n",
              " 'for',\n",
              " 'llms',\n",
              " ',',\n",
              " 'one-shot',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'useful',\n",
              " 'when',\n",
              " 'training',\n",
              " 'data',\n",
              " 'is',\n",
              " 'particularly',\n",
              " 'scarce',\n",
              " 'but',\n",
              " 'the',\n",
              " 'model',\n",
              " 'can',\n",
              " 'generalize',\n",
              " 'well',\n",
              " 'enough',\n",
              " 'from',\n",
              " 'just',\n",
              " 'one',\n",
              " 'labeled',\n",
              " 'example',\n",
              " '.',\n",
              " 'example',\n",
              " ':',\n",
              " 'if',\n",
              " 'you',\n",
              " 'want',\n",
              " 'the',\n",
              " 'model',\n",
              " 'to',\n",
              " 'recognize',\n",
              " 'a',\n",
              " 'new',\n",
              " 'product',\n",
              " 'category',\n",
              " '(',\n",
              " 'like',\n",
              " '``',\n",
              " 'smart',\n",
              " 'thermostats',\n",
              " \"''\",\n",
              " ')',\n",
              " 'and',\n",
              " 'you',\n",
              " 'provide',\n",
              " 'only',\n",
              " 'one',\n",
              " 'example',\n",
              " 'of',\n",
              " 'a',\n",
              " 'product',\n",
              " 'description',\n",
              " 'in',\n",
              " 'this',\n",
              " 'category',\n",
              " ',',\n",
              " 'the',\n",
              " 'model',\n",
              " 'may',\n",
              " 'use',\n",
              " 'that',\n",
              " 'single',\n",
              " 'instance',\n",
              " 'to',\n",
              " 'identify',\n",
              " 'other',\n",
              " 'smart',\n",
              " 'thermostat',\n",
              " 'products',\n",
              " 'based',\n",
              " 'on',\n",
              " 'similarities',\n",
              " 'in',\n",
              " 'language',\n",
              " 'and',\n",
              " 'function',\n",
              " '.',\n",
              " 'multi-shot',\n",
              " 'learning',\n",
              " 'multi-shot',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'similar',\n",
              " 'to',\n",
              " 'few-shot',\n",
              " 'learning',\n",
              " 'but',\n",
              " 'involves',\n",
              " 'more',\n",
              " 'examples',\n",
              " ',',\n",
              " 'which',\n",
              " 'typically',\n",
              " 'improves',\n",
              " 'the',\n",
              " 'model',\n",
              " \"'s\",\n",
              " 'accuracy',\n",
              " 'and',\n",
              " 'generalization',\n",
              " '.',\n",
              " 'this',\n",
              " 'approach',\n",
              " 'requires',\n",
              " 'a',\n",
              " 'set',\n",
              " 'of',\n",
              " 'labeled',\n",
              " 'examples',\n",
              " 'for',\n",
              " 'the',\n",
              " 'model',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'from',\n",
              " ',',\n",
              " 'though',\n",
              " 'it',\n",
              " '’',\n",
              " 's',\n",
              " 'still',\n",
              " 'smaller',\n",
              " 'than',\n",
              " 'the',\n",
              " 'amount',\n",
              " 'required',\n",
              " 'for',\n",
              " 'traditional',\n",
              " 'supervised',\n",
              " 'learning',\n",
              " '.',\n",
              " 'multi-shot',\n",
              " 'learning',\n",
              " 'strikes',\n",
              " 'a',\n",
              " 'balance',\n",
              " 'between',\n",
              " 'extensive',\n",
              " 'training',\n",
              " 'data',\n",
              " 'and',\n",
              " 'the',\n",
              " 'adaptability',\n",
              " 'of',\n",
              " 'fewer',\n",
              " 'examples',\n",
              " '.',\n",
              " 'example',\n",
              " ':',\n",
              " 'imagine',\n",
              " 'training',\n",
              " 'an',\n",
              " 'llm',\n",
              " 'to',\n",
              " 'recognize',\n",
              " 'different',\n",
              " 'dog',\n",
              " 'breeds',\n",
              " '.',\n",
              " 'by',\n",
              " 'showing',\n",
              " 'it',\n",
              " 'several',\n",
              " 'images',\n",
              " 'of',\n",
              " 'a',\n",
              " 'golden',\n",
              " 'retriever',\n",
              " ',',\n",
              " 'the',\n",
              " 'model',\n",
              " 'starts',\n",
              " 'learning',\n",
              " 'the',\n",
              " 'features',\n",
              " 'of',\n",
              " 'this',\n",
              " 'breed',\n",
              " '.',\n",
              " 'with',\n",
              " 'a',\n",
              " 'few',\n",
              " 'more',\n",
              " 'images',\n",
              " 'of',\n",
              " 'similar',\n",
              " 'breeds',\n",
              " ',',\n",
              " 'like',\n",
              " 'labradors',\n",
              " ',',\n",
              " 'it',\n",
              " 'can',\n",
              " 'generalize',\n",
              " 'its',\n",
              " 'knowledge',\n",
              " 'to',\n",
              " 'recognize',\n",
              " 'these',\n",
              " 'as',\n",
              " 'well',\n",
              " ',',\n",
              " 'enabling',\n",
              " 'it',\n",
              " 'to',\n",
              " 'distinguish',\n",
              " 'breeds',\n",
              " 'without',\n",
              " 'needing',\n",
              " 'thousands',\n",
              " 'of',\n",
              " 'examples',\n",
              " '.',\n",
              " 'task',\n",
              " ':',\n",
              " 'question',\n",
              " ':',\n",
              " 'you',\n",
              " 'are',\n",
              " 'part',\n",
              " 'of',\n",
              " 'a',\n",
              " 'team',\n",
              " 'working',\n",
              " 'on',\n",
              " 'an',\n",
              " 'innovative',\n",
              " 'project',\n",
              " 'aiming',\n",
              " 'to',\n",
              " 'adapt',\n",
              " 'a',\n",
              " 'pre',\n",
              " 'trained',\n",
              " 'language',\n",
              " 'model',\n",
              " 'to',\n",
              " 'a',\n",
              " 'new',\n",
              " ',',\n",
              " 'related',\n",
              " 'task',\n",
              " 'without',\n",
              " 'much',\n",
              " 'data',\n",
              " '.',\n",
              " 'to',\n",
              " 'ensure',\n",
              " 'the',\n",
              " 'project',\n",
              " \"'s\",\n",
              " 'success',\n",
              " ',',\n",
              " 'you',\n",
              " 'need',\n",
              " 'to',\n",
              " 'adapt',\n",
              " 'and',\n",
              " 'fine-tune',\n",
              " 'the',\n",
              " 'model',\n",
              " '.',\n",
              " 'which',\n",
              " 'general',\n",
              " 'approach',\n",
              " 'leverages',\n",
              " 'prior',\n",
              " 'knowledge',\n",
              " 'from',\n",
              " 'one',\n",
              " 'task',\n",
              " 'to',\n",
              " 'help',\n",
              " 'train',\n",
              " 'a',\n",
              " 'model',\n",
              " 'on',\n",
              " 'a',\n",
              " 'new',\n",
              " ',',\n",
              " 'related',\n",
              " 'task',\n",
              " '?',\n",
              " 'select',\n",
              " 'one',\n",
              " 'answer',\n",
              " ':',\n",
              " '1.',\n",
              " 'n-shot',\n",
              " 'learning',\n",
              " '2.',\n",
              " 'zero-shot',\n",
              " 'learning',\n",
              " '3.',\n",
              " 'few-shot',\n",
              " 'learning',\n",
              " '4.',\n",
              " 'transfer',\n",
              " 'learning',\n",
              " '5.',\n",
              " 'one-shot',\n",
              " 'learning',\n",
              " 'the',\n",
              " 'correct',\n",
              " 'answer',\n",
              " 'is',\n",
              " ':',\n",
              " '4.',\n",
              " 'transfer',\n",
              " 'learning',\n",
              " 'building',\n",
              " 'blocks',\n",
              " 'to',\n",
              " 'train',\n",
              " 'llms',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSVKOs8819Hs",
        "outputId": "687f4f38-d1b7-4e0d-9020-435104d546a1"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2363"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script builds a vocabulary for the next-word prediction model by assigning unique integer indices to tokens in the dataset, with an unknown token (<UNK>) initialized in the vocabulary."
      ],
      "metadata": {
        "id": "IxVP_nXfjYqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={'<UNK>':0}\n",
        "Counter(tokens).keys\n",
        "for token in Counter(tokens).keys():\n",
        "  if token not in vocab:\n",
        "    vocab[token]=len(vocab)\n",
        "\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIi01Cbl3C07",
        "outputId": "71fcc504-a1fb-40db-959a-74a5317b792b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " 'exploring': 1,\n",
              " 'data': 2,\n",
              " 'availability': 3,\n",
              " 'in': 4,\n",
              " 'llm': 5,\n",
              " 'development': 6,\n",
              " 'when': 7,\n",
              " 'developing': 8,\n",
              " 'a': 9,\n",
              " 'large': 10,\n",
              " 'language': 11,\n",
              " 'model': 12,\n",
              " '(': 13,\n",
              " ')': 14,\n",
              " ',': 15,\n",
              " 'it': 16,\n",
              " \"'s\": 17,\n",
              " 'crucial': 18,\n",
              " 'to': 19,\n",
              " 'consider': 20,\n",
              " 'the': 21,\n",
              " 'of': 22,\n",
              " 'labeled': 23,\n",
              " 'for': 24,\n",
              " 'specific': 25,\n",
              " 'task': 26,\n",
              " 'you': 27,\n",
              " 'want': 28,\n",
              " 'perform': 29,\n",
              " '.': 30,\n",
              " 'an': 31,\n",
              " 'is': 32,\n",
              " 'complex': 33,\n",
              " 'ai': 34,\n",
              " 'trained': 35,\n",
              " 'understand': 36,\n",
              " 'and': 37,\n",
              " 'generate': 38,\n",
              " 'human-like': 39,\n",
              " 'based': 40,\n",
              " 'on': 41,\n",
              " 'patterns': 42,\n",
              " 'learned': 43,\n",
              " 'from': 44,\n",
              " 'vast': 45,\n",
              " 'amounts': 46,\n",
              " 'text': 47,\n",
              " 'however': 48,\n",
              " 'general-purpose': 49,\n",
              " 'llms': 50,\n",
              " 'often': 51,\n",
              " 'need': 52,\n",
              " 'fine-tuning—': 53,\n",
              " 'additional': 54,\n",
              " 'focused': 55,\n",
              " 'training': 56,\n",
              " 'smaller': 57,\n",
              " 'task-specific': 58,\n",
              " 'dataset—to': 59,\n",
              " 'well': 60,\n",
              " 'specialized': 61,\n",
              " 'like': 62,\n",
              " 'summarizing': 63,\n",
              " 'scientific': 64,\n",
              " 'articles': 65,\n",
              " 'or': 66,\n",
              " 'answering': 67,\n",
              " 'customer': 68,\n",
              " 'support': 69,\n",
              " 'queries': 70,\n",
              " 'fine-tuning': 71,\n",
              " 'adapting': 72,\n",
              " 'new': 73,\n",
              " 'particularly': 74,\n",
              " 'valuable': 75,\n",
              " 'limited': 76,\n",
              " 'such': 77,\n",
              " 'cases': 78,\n",
              " 'with': 79,\n",
              " 'targeted': 80,\n",
              " 'datasets': 81,\n",
              " 'allows': 82,\n",
              " 'tasks': 83,\n",
              " 'effectively': 84,\n",
              " 'minimal': 85,\n",
              " 'methods': 86,\n",
              " 'zero-shot': 87,\n",
              " 'few-shot': 88,\n",
              " 'multi-shot': 89,\n",
              " 'learning—referred': 90,\n",
              " 'collectively': 91,\n",
              " 'as': 92,\n",
              " 'n-shot': 93,\n",
              " 'learning—become': 94,\n",
              " 'essential': 95,\n",
              " 'adapt': 96,\n",
              " 'understanding': 97,\n",
              " 'transfer': 98,\n",
              " 'learning': 99,\n",
              " 'powerful': 100,\n",
              " 'approach': 101,\n",
              " 'that': 102,\n",
              " 'enables': 103,\n",
              " 'models': 104,\n",
              " 'one': 105,\n",
              " 'apply': 106,\n",
              " 'knowledge': 107,\n",
              " 'they': 108,\n",
              " 'gained': 109,\n",
              " 'related': 110,\n",
              " 'but': 111,\n",
              " 'different': 112,\n",
              " 'this': 113,\n",
              " 'similar': 114,\n",
              " 'how': 115,\n",
              " 'humans': 116,\n",
              " 'can': 117,\n",
              " 'across': 118,\n",
              " 'skills': 119,\n",
              " 'example': 120,\n",
              " 'musician': 121,\n",
              " 'piano': 122,\n",
              " 'reading': 123,\n",
              " 'music': 124,\n",
              " 'rhythm': 125,\n",
              " 'guitar': 126,\n",
              " 'context': 127,\n",
              " 'broad': 128,\n",
              " 'general': 129,\n",
              " 'news': 130,\n",
              " 'books': 131,\n",
              " 'websites': 132,\n",
              " 'its': 133,\n",
              " 'require': 134,\n",
              " 'medical': 135,\n",
              " 'legal': 136,\n",
              " 'processing': 137,\n",
              " 'lets': 138,\n",
              " 'reuse': 139,\n",
              " 'base': 140,\n",
              " 'semantics': 141,\n",
              " 'which': 142,\n",
              " 'may': 143,\n",
              " 'not': 144,\n",
              " 'have': 145,\n",
              " 'technique': 146,\n",
              " 'haven': 147,\n",
              " '’': 148,\n",
              " 't': 149,\n",
              " 'explicitly': 150,\n",
              " 'works': 151,\n",
              " 'by': 152,\n",
              " 'leveraging': 153,\n",
              " 'scenarios': 154,\n",
              " 'imagine': 155,\n",
              " 'child': 156,\n",
              " 'who': 157,\n",
              " 'has': 158,\n",
              " 'never': 159,\n",
              " 'seen': 160,\n",
              " 'zebra': 161,\n",
              " 'knows': 162,\n",
              " 'what': 163,\n",
              " 'horse': 164,\n",
              " 'looks': 165,\n",
              " 'if': 166,\n",
              " 'someone': 167,\n",
              " 'tells': 168,\n",
              " '``': 169,\n",
              " 'striped': 170,\n",
              " \"''\": 171,\n",
              " 'identify': 172,\n",
              " 'without': 173,\n",
              " 'any': 174,\n",
              " 'similarly': 175,\n",
              " 'variety': 176,\n",
              " 'use': 177,\n",
              " 'answer': 178,\n",
              " 'questions': 179,\n",
              " 'about': 180,\n",
              " 'topics': 181,\n",
              " 'hasn': 182,\n",
              " 'directly': 183,\n",
              " 'making': 184,\n",
              " 'educated': 185,\n",
              " 'guesses': 186,\n",
              " ':': 187,\n",
              " 'suppose': 188,\n",
              " 'asked': 189,\n",
              " 'translate': 190,\n",
              " 'sentence': 191,\n",
              " 'into': 192,\n",
              " 'been': 193,\n",
              " 'similarities': 194,\n",
              " 'other': 195,\n",
              " 'languages': 196,\n",
              " 'might': 197,\n",
              " 'approximate': 198,\n",
              " 'translation': 199,\n",
              " 'some': 200,\n",
              " 'level': 201,\n",
              " 'accuracy': 202,\n",
              " 'even': 203,\n",
              " 'having': 204,\n",
              " 'learn': 205,\n",
              " 'only': 206,\n",
              " 'few': 207,\n",
              " 'examples': 208,\n",
              " 'relies': 209,\n",
              " 's': 210,\n",
              " 'ability': 211,\n",
              " 'generalize': 212,\n",
              " 'previous': 213,\n",
              " 'more': 214,\n",
              " 'adaptable': 215,\n",
              " 'ones': 216,\n",
              " 'instance': 217,\n",
              " 'student': 218,\n",
              " 'attended': 219,\n",
              " 'lectures': 220,\n",
              " 'topic': 221,\n",
              " 'exam': 222,\n",
              " 'question': 223,\n",
              " 'class': 224,\n",
              " 'much': 225,\n",
              " 'study': 226,\n",
              " 'just': 227,\n",
              " 'small': 228,\n",
              " 'number': 229,\n",
              " 'structure': 230,\n",
              " 'given': 231,\n",
              " 'three': 232,\n",
              " 'four': 233,\n",
              " 'summarize': 234,\n",
              " 'still': 235,\n",
              " 'enough': 236,\n",
              " 'using': 237,\n",
              " 'those': 238,\n",
              " 'infer': 239,\n",
              " 'rules': 240,\n",
              " 'summarization': 241,\n",
              " 'one-shot': 242,\n",
              " 'part': 243,\n",
              " 'case': 244,\n",
              " 'requires': 245,\n",
              " 'single': 246,\n",
              " 'teach': 247,\n",
              " 'sees': 248,\n",
              " 'solve': 249,\n",
              " 'math': 250,\n",
              " 'problem': 251,\n",
              " 'then': 252,\n",
              " 'problems': 253,\n",
              " 'their': 254,\n",
              " 'own': 255,\n",
              " 'useful': 256,\n",
              " 'scarce': 257,\n",
              " 'recognize': 258,\n",
              " 'product': 259,\n",
              " 'category': 260,\n",
              " 'smart': 261,\n",
              " 'thermostats': 262,\n",
              " 'provide': 263,\n",
              " 'description': 264,\n",
              " 'thermostat': 265,\n",
              " 'products': 266,\n",
              " 'function': 267,\n",
              " 'involves': 268,\n",
              " 'typically': 269,\n",
              " 'improves': 270,\n",
              " 'generalization': 271,\n",
              " 'set': 272,\n",
              " 'though': 273,\n",
              " 'than': 274,\n",
              " 'amount': 275,\n",
              " 'required': 276,\n",
              " 'traditional': 277,\n",
              " 'supervised': 278,\n",
              " 'strikes': 279,\n",
              " 'balance': 280,\n",
              " 'between': 281,\n",
              " 'extensive': 282,\n",
              " 'adaptability': 283,\n",
              " 'fewer': 284,\n",
              " 'dog': 285,\n",
              " 'breeds': 286,\n",
              " 'showing': 287,\n",
              " 'several': 288,\n",
              " 'images': 289,\n",
              " 'golden': 290,\n",
              " 'retriever': 291,\n",
              " 'starts': 292,\n",
              " 'features': 293,\n",
              " 'breed': 294,\n",
              " 'labradors': 295,\n",
              " 'these': 296,\n",
              " 'enabling': 297,\n",
              " 'distinguish': 298,\n",
              " 'needing': 299,\n",
              " 'thousands': 300,\n",
              " 'are': 301,\n",
              " 'team': 302,\n",
              " 'working': 303,\n",
              " 'innovative': 304,\n",
              " 'project': 305,\n",
              " 'aiming': 306,\n",
              " 'pre': 307,\n",
              " 'ensure': 308,\n",
              " 'success': 309,\n",
              " 'fine-tune': 310,\n",
              " 'leverages': 311,\n",
              " 'prior': 312,\n",
              " 'help': 313,\n",
              " 'train': 314,\n",
              " '?': 315,\n",
              " 'select': 316,\n",
              " '1.': 317,\n",
              " '2.': 318,\n",
              " '3.': 319,\n",
              " '4.': 320,\n",
              " '5.': 321,\n",
              " 'correct': 322,\n",
              " 'building': 323,\n",
              " 'blocks': 324,\n",
              " 'section': 325,\n",
              " 'we': 326,\n",
              " 'focuses': 327,\n",
              " 'two': 328,\n",
              " 'core': 329,\n",
              " 'techniques': 330,\n",
              " 'pre-train': 331,\n",
              " '—': 332,\n",
              " 'next': 333,\n",
              " 'word': 334,\n",
              " 'prediction': 335,\n",
              " 'masked': 336,\n",
              " 'modeling': 337,\n",
              " 'serve': 338,\n",
              " 'foundational': 339,\n",
              " 'steps': 340,\n",
              " 'many': 341,\n",
              " 'advanced': 342,\n",
              " 'including': 343,\n",
              " 'used': 344,\n",
              " 'natural': 345,\n",
              " 'nlp': 346,\n",
              " 'pre-training': 347,\n",
              " 'massive': 348,\n",
              " 'dataset': 349,\n",
              " 'give': 350,\n",
              " 'before': 351,\n",
              " 'fine-tuned': 352,\n",
              " 'although': 353,\n",
              " 'scratch': 354,\n",
              " 'be': 355,\n",
              " 'costly': 356,\n",
              " 'time-consuming': 357,\n",
              " 'organizations': 358,\n",
              " 'pre-existing': 359,\n",
              " 'pre-trained': 360,\n",
              " 'instead': 361,\n",
              " 'them': 362,\n",
              " 'particular': 363,\n",
              " 'needs': 364,\n",
              " 'generative': 365,\n",
              " 'where': 366,\n",
              " 'sequences': 367,\n",
              " 'words': 368,\n",
              " 'tokens': 369,\n",
              " 'learns': 370,\n",
              " 'predict': 371,\n",
              " 'token': 372,\n",
              " 'sequence': 373,\n",
              " 'through': 374,\n",
              " 'repeated': 375,\n",
              " 'exposure': 376,\n",
              " 'coherent': 377,\n",
              " 'contextually': 378,\n",
              " 'relevant': 379,\n",
              " 'process': 380,\n",
              " 'lays': 381,\n",
              " 'groundwork': 382,\n",
              " 'produce': 383,\n",
              " 'main': 384,\n",
              " 'types': 385,\n",
              " 'both': 386,\n",
              " 'allow': 387,\n",
              " 'relationships': 388,\n",
              " 'contextual': 389,\n",
              " 'meaning': 390,\n",
              " 'come': 391,\n",
              " 'data—in': 392,\n",
              " 'sentences': 393,\n",
              " 'processes': 394,\n",
              " 'each': 395,\n",
              " 'builds': 396,\n",
              " 'follow': 397,\n",
              " 'another': 398,\n",
              " '“': 399,\n",
              " 'quick': 400,\n",
              " 'brown': 401,\n",
              " 'fox': 402,\n",
              " 'jumps': 403,\n",
              " 'over': 404,\n",
              " 'lazy': 405,\n",
              " '”': 406,\n",
              " 'input': 407,\n",
              " 'most': 408,\n",
              " 'likely': 409,\n",
              " 'after': 410,\n",
              " 'correctly': 411,\n",
              " 'predicting': 412,\n",
              " 'added': 413,\n",
              " 'creating': 414,\n",
              " 'tries': 415,\n",
              " 'jumps.': 416,\n",
              " 'continues': 417,\n",
              " 'helping': 418,\n",
              " 'capture': 419,\n",
              " 'dependencies': 420,\n",
              " 'improve': 421,\n",
              " 'at': 422,\n",
              " 'generating': 423,\n",
              " 'prompt': 424,\n",
              " 'i': 425,\n",
              " 'drink': 426,\n",
              " 'coffee': 427,\n",
              " '__': 428,\n",
              " 'during': 429,\n",
              " 'will': 430,\n",
              " 'morning': 431,\n",
              " 'common': 432,\n",
              " 'association': 433,\n",
              " 'routines': 434,\n",
              " 'create': 435,\n",
              " 'numerous': 436,\n",
              " 'input-output': 437,\n",
              " 'pairs': 438,\n",
              " 'output': 439,\n",
              " 'back': 440,\n",
              " 'longer': 441,\n",
              " 'look': 442,\n",
              " '\\uf0b7': 443,\n",
              " '→': 444,\n",
              " 'begins': 445,\n",
              " 'associations': 446,\n",
              " 'prompted': 447,\n",
              " 'eat': 448,\n",
              " 'pizza': 449,\n",
              " 'cheese': 450,\n",
              " 'rather': 451,\n",
              " 'oregano': 452,\n",
              " 'ketchup': 453,\n",
              " 'because': 454,\n",
              " 'frequently': 455,\n",
              " 'appears': 456,\n",
              " 'contexts': 457,\n",
              " 'type': 458,\n",
              " 'accurate': 459,\n",
              " 'realistic': 460,\n",
              " 'mlm': 461,\n",
              " 'popular': 462,\n",
              " 'hidden': 463,\n",
              " 'within': 464,\n",
              " 'challenges': 465,\n",
              " 'missing': 466,\n",
              " 'information': 467,\n",
              " 'surrounding': 468,\n",
              " 'clues': 469,\n",
              " 'develop': 470,\n",
              " 'nuanced': 471,\n",
              " 'randomly': 472,\n",
              " 'replaced': 473,\n",
              " '[': 474,\n",
              " 'mask': 475,\n",
              " ']': 476,\n",
              " 'so': 477,\n",
              " 'becomes': 478,\n",
              " 'dog.': 479,\n",
              " 'analyzing': 480,\n",
              " 'provided': 481,\n",
              " 'rest': 482,\n",
              " 'could': 483,\n",
              " 'theoretically': 484,\n",
              " 'adjectives': 485,\n",
              " 'option': 486,\n",
              " 'here': 487,\n",
              " 'encounters': 488,\n",
              " 'enjoy': 489,\n",
              " 'weekends.': 490,\n",
              " 'since': 491,\n",
              " 'weekends': 492,\n",
              " 'phrase': 493,\n",
              " 'helps': 494,\n",
              " 'better': 495,\n",
              " 'sense': 496,\n",
              " 'sales': 497,\n",
              " 'company': 498,\n",
              " 'explain': 499,\n",
              " 'business': 500,\n",
              " 'stakeholders': 501,\n",
              " 'present': 502,\n",
              " 'sample': 503,\n",
              " 'illustrate': 504,\n",
              " 'quickly': 505,\n",
              " 'resolved': 506,\n",
              " 'possible': 507,\n",
              " 'options': 508,\n",
              " 'office': 509,\n",
              " 'manager': 510,\n",
              " 'fight': 511,\n",
              " 'work': 512,\n",
              " 'bone': 513,\n",
              " 'agent': 514,\n",
              " 'issue': 515,\n",
              " 'station': 516,\n",
              " 'officer': 517,\n",
              " 'feedback': 518,\n",
              " 'following': 519,\n",
              " 'weather': 520,\n",
              " '……': 521,\n",
              " 'would': 522,\n",
              " '1': 523,\n",
              " 'cupboard': 524,\n",
              " '2': 525,\n",
              " 'today': 526,\n",
              " '3': 527,\n",
              " 'do': 528,\n",
              " \"n't\": 529,\n",
              " 'know': 530,\n",
              " '4': 531,\n",
              " 'rainy': 532,\n",
              " 'scientist': 533,\n",
              " 'planning': 534,\n",
              " 'generic': 535,\n",
              " 'applications': 536,\n",
              " 'organization': 537,\n",
              " 'anticipates': 538,\n",
              " 'also': 539,\n",
              " 'intends': 540,\n",
              " 'build': 541,\n",
              " 'service': 542,\n",
              " 'bot': 543,\n",
              " 'address': 544,\n",
              " 'high': 545,\n",
              " 'volume': 546,\n",
              " 'optimal': 547,\n",
              " 'performance': 548,\n",
              " 'ai-driven': 549,\n",
              " 'chatbot': 550,\n",
              " 'expected': 551,\n",
              " 'combination': 552,\n",
              " 'order': 553,\n",
              " 'arrange': 554,\n",
              " 'should': 555,\n",
              " 'tokenize': 556,\n",
              " 'remove': 557,\n",
              " 'stop': 558,\n",
              " 'lemmatize': 559,\n",
              " 'raw': 560,\n",
              " 'embeddings': 561,\n",
              " 'convert': 562,\n",
              " 'numbers': 563,\n",
              " '–': 564,\n",
              " 'start': 565,\n",
              " 'preprocessing': 566,\n",
              " 'clean': 567,\n",
              " 'standardized': 568,\n",
              " 'cleaned': 569,\n",
              " 'numerical': 570,\n",
              " 'representations': 571,\n",
              " 'allowing': 572,\n",
              " 'finally': 573}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G1hWDFD3R0K",
        "outputId": "e37e9349-4cc6-4d07-c875-73754831018b"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "574"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script splits the input document into individual sentences, preparing the text data for further processing in the next-word prediction model."
      ],
      "metadata": {
        "id": "6ouJjde4jhRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences=document.split('\\n')"
      ],
      "metadata": {
        "id": "naOkOaDx42hX"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pD_f9om25uPk",
        "outputId": "48b2d1c6-a050-4dff-e61c-c01e7d99d78e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Exploring Data Availability in LLM Development ',\n",
              " \"When developing a large language model (LLM), it's crucial to consider the availability of \",\n",
              " 'labeled data for the specific task you want the model to perform. An LLM is a complex AI ',\n",
              " 'model trained to understand and generate human-like language based on patterns learned ',\n",
              " 'from vast amounts of text data. However, general-purpose LLMs often need fine-tuning—',\n",
              " ' additional, focused training on a smaller, task-specific dataset—to perform well on a ',\n",
              " 'specialized task, like summarizing scientific articles or answering customer support queries. ',\n",
              " 'Fine-tuning, or adapting a model to a new task, is particularly valuable when data is limited. ',\n",
              " 'In such cases, fine-tuning the model with smaller, targeted datasets allows it to perform ',\n",
              " 'specialized tasks effectively. When labeled data is minimal, methods like zero-shot, few-shot, ',\n",
              " 'and multi-shot learning—referred to collectively as N-shot learning—become essential to ',\n",
              " 'adapt the model. ',\n",
              " 'Understanding Transfer Learning ',\n",
              " 'Transfer learning is a powerful AI approach that enables models trained on one task to apply ',\n",
              " 'the knowledge they gained to a related but different task. This is similar to how humans can ',\n",
              " 'transfer knowledge across skills. For example, a musician trained in piano can transfer skills ',\n",
              " 'like reading music and understanding rhythm to learning the guitar. In the context of LLMs, a ',\n",
              " 'model trained on broad, general text (like news articles, books, and websites) can transfer its ',\n",
              " 'language understanding to tasks that require specialized knowledge, like medical or legal ',\n",
              " 'language processing. Transfer learning lets the model reuse its base knowledge of language ',\n",
              " 'and semantics to perform well on a task for which it may not have specific training data. ',\n",
              " 'Zero-shot Learning ',\n",
              " 'Zero-shot learning is a technique that allows LLMs to perform tasks they haven’t explicitly ',\n",
              " \"trained for. It works by leveraging the model's broad understanding of language and context \",\n",
              " 'to apply this knowledge to new scenarios. Imagine a child who has never seen a zebra but ',\n",
              " 'knows what a horse looks like. If someone tells the child that a zebra looks like a \"striped ',\n",
              " 'horse,\" the child can identify the zebra without any specific training. Similarly, an LLM ',\n",
              " 'trained on a variety of text can use zero-shot learning to answer questions about topics it ',\n",
              " 'hasn’t directly learned by making educated guesses based on its general language ',\n",
              " 'understanding. ',\n",
              " 'Example: ',\n",
              " 'Suppose an LLM is asked to translate a sentence into a language it hasn’t been trained on ',\n",
              " 'directly. If the model has learned similarities and patterns in other languages, it might ',\n",
              " 'approximate the translation with some level of accuracy, even without having any data on ',\n",
              " 'that specific language. ',\n",
              " 'Few-shot Learning ',\n",
              " 'Few-shot learning allows a model to learn a new task with only a few examples. This ',\n",
              " 'approach relies on the model’s ability to generalize from previous tasks, making it more ',\n",
              " 'adaptable to new ones even with limited examples. For instance, a student who has attended ',\n",
              " 'lectures on a topic might answer an exam question based on what they learned in class ',\n",
              " 'without much additional study. Few-shot learning similarly enables LLMs to perform a new ',\n",
              " 'task effectively with just a small number of training examples. ',\n",
              " 'Example: ',\n",
              " 'If an LLM has been trained to understand language structure and is given only three or four ',\n",
              " 'labeled examples of how to summarize news articles, it can still generalize well enough to ',\n",
              " 'summarize new articles by using those few examples to infer the general rules of ',\n",
              " 'summarization. ',\n",
              " 'One-shot Learning as Part of Few-shot Learning ',\n",
              " 'A specific case of few-shot learning, one-shot learning, requires only a single example to ',\n",
              " 'teach the model a task. For example, suppose a student sees one example of how to solve a ',\n",
              " 'math problem. They might then apply that single example to solve similar problems on their ',\n",
              " 'own. For LLMs, one-shot learning is useful when training data is particularly scarce but the ',\n",
              " 'model can generalize well enough from just one labeled example. ',\n",
              " 'Example: ',\n",
              " 'If you want the model to recognize a new product category (like \"smart thermostats\") and ',\n",
              " 'you provide only one example of a product description in this category, the model may use ',\n",
              " 'that single instance to identify other smart thermostat products based on similarities in ',\n",
              " 'language and function. ',\n",
              " 'Multi-shot Learning ',\n",
              " 'Multi-shot learning is similar to few-shot learning but involves more examples, which ',\n",
              " \"typically improves the model's accuracy and generalization. This approach requires a set of \",\n",
              " 'labeled examples for the model to learn from, though it’s still smaller than the amount ',\n",
              " 'required for traditional supervised learning. Multi-shot learning strikes a balance between ',\n",
              " 'extensive training data and the adaptability of fewer examples. ',\n",
              " 'Example: ',\n",
              " 'Imagine training an LLM to recognize different dog breeds. By showing it several images of ',\n",
              " 'a Golden Retriever, the model starts learning the features of this breed. With a few more ',\n",
              " 'images of similar breeds, like Labradors, it can generalize its knowledge to recognize these as ',\n",
              " 'well, enabling it to distinguish breeds without needing thousands of examples. ',\n",
              " 'Task: ',\n",
              " 'Question: You are part of a team working on an innovative project aiming to adapt a pre',\n",
              " \"trained language model to a new, related task without much data. To ensure the project's \",\n",
              " 'success, you need to adapt and fine-tune the model. Which general approach leverages prior ',\n",
              " 'knowledge from one task to help train a model on a new, related task? ',\n",
              " 'Select one answer: ',\n",
              " '1. N-shot learning ',\n",
              " '2. Zero-shot learning ',\n",
              " '3. Few-shot learning ',\n",
              " '4. Transfer learning ',\n",
              " '5. One-shot learning ',\n",
              " 'The correct answer is: ',\n",
              " '4. Transfer learning ',\n",
              " 'Building Blocks to Train LLMs ',\n",
              " 'In this section we focuses on two core techniques to pre-train large language models (LLMs) ',\n",
              " '— next word prediction and masked language modeling. These methods serve as ',\n",
              " 'foundational steps in training many advanced language models, including those used in ',\n",
              " 'natural language processing (NLP) tasks. Pre-training a model involves using a massive ',\n",
              " 'dataset to give the model a general understanding of language before it’s fine-tuned for ',\n",
              " 'specific tasks. Although pre-training from scratch can be costly and time-consuming, many ',\n",
              " 'organizations fine-tune pre-existing pre-trained models instead, adapting them to their ',\n",
              " 'particular needs. ',\n",
              " 'Generative Pre-Training  ',\n",
              " 'Generative pre-training is a technique where the model is given sequences of words or text ',\n",
              " 'tokens and learns to predict the next token in that sequence. Through repeated exposure to ',\n",
              " 'different text sequences, the model learns to generate language that is coherent and ',\n",
              " 'contextually relevant. This pre-training process lays the groundwork for the model’s ability to ',\n",
              " 'understand and produce natural language. Two main types of generative pre-training ',\n",
              " 'techniques are next word prediction and masked language modeling, both of which allow the ',\n",
              " 'model to learn patterns, relationships, and the contextual meaning of words. ',\n",
              " '1. Next Word Prediction ',\n",
              " 'Next word prediction is a supervised learning technique where the model is trained to predict ',\n",
              " 'the next word in a sequence based on the words that come before it. In supervised learning, ',\n",
              " 'the model learns from labeled data—in this case, sentences with a specific sequence of ',\n",
              " 'words. As the model processes each word in a sentence, it builds a contextual understanding ',\n",
              " 'of how words typically follow one another. ',\n",
              " 'For example, in the sentence “The quick brown fox jumps over the lazy dog,” the model ',\n",
              " 'might be given the input “The quick brown” and be trained to predict the word “fox” as the ',\n",
              " 'most likely next word. After correctly predicting “fox,” this word is added to the input ',\n",
              " 'sequence, creating “The quick brown fox,” and the model then tries to predict “jumps.” This ',\n",
              " 'process continues, with each prediction added to the sequence, helping the model capture ',\n",
              " 'dependencies between words and improve at generating coherent text. Suppose you give the ',\n",
              " 'model a prompt, like \"I like to drink coffee in the __.\" The model, having seen many similar ',\n",
              " 'sentences during training, will likely predict \"morning\" as the next word based on the ',\n",
              " 'common association between coffee and morning routines. ',\n",
              " 'Training Data for Next Word Prediction ',\n",
              " 'To train the model, large datasets are used to create numerous input-output pairs. Each output ',\n",
              " 'is then added back into the sequence for the next input, helping the model learn longer ',\n",
              " 'patterns and more complex word dependencies. Using a single sentence, like “The quick ',\n",
              " 'brown fox jumps over the lazy dog,” training pairs might look like this: ',\n",
              " '\\uf0b7 Input: “The quick brown” → Output: “fox” ',\n",
              " '\\uf0b7 Input: “The quick brown fox” → Output: “jumps” ',\n",
              " '\\uf0b7 Input: “The quick brown fox jumps” → Output: “over” ',\n",
              " 'Through many such examples, the model begins to understand common word associations. ',\n",
              " 'For instance, when prompted with “I like to eat pizza with __,” it might predict “cheese” ',\n",
              " 'rather than words like “oregano” or “ketchup,” because it has learned that “cheese” ',\n",
              " 'frequently appears with “pizza” in similar contexts. This type of learning lets the model ',\n",
              " 'generate more accurate and realistic sentences. ',\n",
              " '2. Masked Language Modeling ',\n",
              " 'Masked language modeling (MLM) is another popular technique for generative pre-training, ',\n",
              " 'but instead of predicting the next word in a sequence, it involves predicting a word that has ',\n",
              " 'been “masked” or hidden within a sentence. This approach challenges the model to infer ',\n",
              " 'missing information from surrounding words, helping it learn contextual clues and develop a ',\n",
              " 'nuanced understanding of language. ',\n",
              " 'In MLM, a word within a sentence is randomly replaced with a “[MASK]” token. For ',\n",
              " 'example, in the sentence “The quick brown fox jumps over the lazy dog,” the word “brown” ',\n",
              " 'might be masked, so the input becomes “The quick [MASK] fox jumps over the lazy dog.” ',\n",
              " 'The model is trained to predict the missing word (“brown”) by analyzing the context ',\n",
              " 'provided by the rest of the sentence. Even though “brown” could theoretically be replaced by ',\n",
              " 'many different adjectives, the model learns through training data that “brown” is the most ',\n",
              " 'likely option here. Suppose the model encounters the sentence “I enjoy reading books on ',\n",
              " '[MASK] weekends.” Based on its prior training, the model will likely predict “the” as the ',\n",
              " 'masked word, since “on the weekends” is a common phrase structure. This ability to predict ',\n",
              " 'missing words based on context helps the model develop a better sense of language structure ',\n",
              " 'and word relationships. ',\n",
              " 'Task: ',\n",
              " \"Question: As part of a sales company's AI development team, you have been asked to \",\n",
              " 'explain how masked language modeling works to business stakeholders. You present a ',\n",
              " 'sample of masked data to help illustrate this pre-training process: ',\n",
              " 'Sample: \"The [MASK] support [MASK] quickly resolved the [MASK].\" ',\n",
              " 'What words have been masked? ',\n",
              " 'Possible Options: ',\n",
              " '1. office, manager, fight ',\n",
              " '2. work, dog, bone ',\n",
              " '3. customer, agent, issue ',\n",
              " '4. station, officer, feedback ',\n",
              " 'Correct Answer: ',\n",
              " '3. customer, agent, issue ',\n",
              " 'Question: ',\n",
              " 'You have been working on training an LLM using next word prediction. You have provided ',\n",
              " 'the model with the following training data to help it learn how to predict the next word: ',\n",
              " '\\uf0b7 What is ',\n",
              " '\\uf0b7 What is the ',\n",
              " '\\uf0b7 What is the weather ',\n",
              " '\\uf0b7 What is the weather like ',\n",
              " '\\uf0b7 What is the weather like …… ',\n",
              " 'Which would be the correct prediction for the next word(s)? ',\n",
              " 'Possible Options: ',\n",
              " '1. \"in the cupboard?\" ',\n",
              " '2. \"today?\" ',\n",
              " '3. \"I don\\'t know.\" ',\n",
              " '4. \"rainy?\" ',\n",
              " 'Correct Answer: ',\n",
              " '2. \"today?\" ',\n",
              " 'Question: ',\n",
              " 'You are a data scientist planning to develop large language models from scratch, which ',\n",
              " 'involves building a large generic model for different applications the organization anticipates. ',\n",
              " 'The organization also intends to build a customer service bot to address the high volume of ',\n",
              " 'customer queries. To ensure optimal performance of their AI-driven chatbot, you are ',\n",
              " 'expected to use a combination of techniques in a specific order. ',\n",
              " 'Arrange the techniques in the order the company should use them for their language ',\n",
              " 'model: ',\n",
              " '1. Tokenize, remove stop words, and lemmatize the raw text ',\n",
              " '2. Generate word embeddings to convert language to numbers ',\n",
              " '3. Train the model using masked language modeling ',\n",
              " '4. Fine-tune the model using task-specific data ',\n",
              " 'Answer: ',\n",
              " '1. Tokenize, remove stop words, and lemmatize the raw text – Start by preprocessing the ',\n",
              " 'text data to ensure that the raw text is clean and standardized. ',\n",
              " '2. Generate word embeddings to convert language to numbers – Convert the cleaned text ',\n",
              " 'into numerical representations that the model can process. ',\n",
              " '3. Train the model using masked language modeling – Use masked language modeling ',\n",
              " 'to pre-train the model on a large dataset, allowing it to understand language structure ',\n",
              " 'and context. ',\n",
              " '4. Fine-tune the model using task-specific data – Finally, adapt the pre-trained model to ',\n",
              " 'the specific customer service task by fine-tuning it on relevant labeled data. ']"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function converts a sentence into a sequence of numerical indices based on the vocabulary, using the <UNK> token for unknown words not found in the vocabulary."
      ],
      "metadata": {
        "id": "pc1gw4zljnt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_indices(sentence, vocab):\n",
        "  numerical_sentence=[]\n",
        "  for token in sentence:\n",
        "    if token not in vocab:\n",
        "      numerical_sentence.append(vocab['<UNK>'])\n",
        "    else:\n",
        "      numerical_sentence.append(vocab[token])\n",
        "  return numerical_sentence"
      ],
      "metadata": {
        "id": "Ya2dByIG7FD0"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script converts the list of input sentences into sequences of numerical indices by tokenizing each sentence, converting to lowercase, and mapping tokens to their corresponding indices in the vocabulary."
      ],
      "metadata": {
        "id": "UYczHb7CjtsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_numerical_sentences = []\n",
        "\n",
        "for sentence in input_sentences:\n",
        "  input_numerical_sentences.append(text_indices(word_tokenize(sentence.lower()), vocab))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8NBeibG36F18"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_numerical_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4vBcvem6iJf",
        "outputId": "4f338e62-c01f-4a64-cf42-2fcfac21d1ee"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "195"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script generates training sequences for the next-word prediction model by creating subsequences from each sentence, where each sequence includes progressively more tokens to predict the next word."
      ],
      "metadata": {
        "id": "QMZheV86jzpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequences = []\n",
        "for sentence in input_numerical_sentences:\n",
        "  for i in range(1,len(sentence)):\n",
        "    training_sequences.append(sentence[:i+1])\n",
        ""
      ],
      "metadata": {
        "collapsed": true,
        "id": "C2wgMmiy83OP"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLjJSb4N8-SF",
        "outputId": "a9d56072-0467-493d-8746-6abfc77665a6"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2169"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequences[:9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovkcTpnp_P6L",
        "outputId": "3c631f1e-9e4b-4c62-8100-fe55508e1d61"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2],\n",
              " [1, 2, 3],\n",
              " [1, 2, 3, 4],\n",
              " [1, 2, 3, 4, 5],\n",
              " [1, 2, 3, 4, 5, 6],\n",
              " [7, 8],\n",
              " [7, 8, 9],\n",
              " [7, 8, 9, 10],\n",
              " [7, 8, 9, 10, 11]]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script calculates the lengths of all training sequences and finds the maximum sequence length, which can be useful for padding or defining input size for the model."
      ],
      "metadata": {
        "id": "oM5rZG_Lj6UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len_list=[]\n",
        "for sequence in training_sequences:\n",
        "  len_list.append(len(sequence))\n",
        "\n",
        "max(len_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0iJKIltIvuR",
        "outputId": "970bae7a-2a52-429a-814f-d9a9814f6e89"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script pads the training sequences with zeros to ensure they all have the same length, based on the maximum sequence length, preparing the data for input into the model."
      ],
      "metadata": {
        "id": "b0RwxRnhj_Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence=[]\n",
        "for sequence in training_sequences:\n",
        " padded_training_sequence.append([0]*(max(len_list)-len(sequence))+sequence)"
      ],
      "metadata": {
        "id": "mh6BdhlZJmPi"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(padded_training_sequence[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWesUsiyJy8X",
        "outputId": "9eeda916-decb-44c7-dbb7-8ac81c318176"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script converts the padded training sequences into a PyTorch tensor of type long, making it ready for training in a deep learning model."
      ],
      "metadata": {
        "id": "5Fe8BK-vkE9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence=torch.tensor(padded_training_sequence, dtype=torch.long)"
      ],
      "metadata": {
        "id": "5ZqhOL5JK9F7"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS1TN0IgLz6d",
        "outputId": "21d1fcf1-2592-4fb8-8c0e-c188f32f531f"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2169, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script splits the padded training sequences into input (x) and target (y) tensors, where x contains all tokens except the last one (input sequence), and y contains the last token (target word to predict)."
      ],
      "metadata": {
        "id": "7e1Ro8B9kKSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=padded_training_sequence[:, :-1]\n",
        "y=padded_training_sequence[:, -1]"
      ],
      "metadata": {
        "id": "Vwyl426rL0lq"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITQrdpBJMHc8",
        "outputId": "4a1e2b61-21f8-4d25-d9c0-e0698dfc05f2"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2169, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, dataloader"
      ],
      "metadata": {
        "id": "NLz6_ZWwMIaz"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script defines a custom PyTorch dataset class that stores the input (x) and target (y) sequences, enabling easy batching and access to training data during model training."
      ],
      "metadata": {
        "id": "VJvYM0bKkQ0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "VhL7znQLMLCk"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script creates a custom dataset instance, dataset, using the input (x) and target (y) tensors, ready for use in data loading and model training."
      ],
      "metadata": {
        "id": "HmW7i08LkWiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=CustomDataset(x,y)"
      ],
      "metadata": {
        "id": "mJS0KNbLNhxC"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sniE2A1TNzIp",
        "outputId": "227997fb-b488-4c3a-cc1f-51428ed4cfe4"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2169"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script creates a DataLoader for the custom dataset, enabling efficient batch processing and shuffling of the training data with a batch size of 32."
      ],
      "metadata": {
        "id": "AQWR6wTSkd-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader=DataLoader(dataset=dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "Xv-5L7xWN93s"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w-2pI7LOBWN",
        "outputId": "6e2feb6d-cece-4121-e0a4-d3526ec0634f"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2]),\n",
              " tensor(3))"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script defines an LSTM-based model for next-word prediction, using an embedding layer for token representation, an LSTM layer for sequence learning, and a fully connected layer for generating predictions based on the final hidden state."
      ],
      "metadata": {
        "id": "MOHYqG0ZkkBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMmodel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.emmbedding=nn.Embedding(vocab_size, 100)\n",
        "    self.lstm=nn.LSTM(100, 150, batch_first=True)\n",
        "    self.fc=nn.Linear(150, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    embedded=self.emmbedding(x)\n",
        "    intermediate_hidden_state, (final_hidden_state, final_cell_state)=self.lstm(embedded)\n",
        "    output= self.fc(final_hidden_state.squeeze(0))\n",
        "    return output"
      ],
      "metadata": {
        "id": "nShzHfEzOiEY"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script creates an instance of the LSTM model, initializing it with the vocabulary size to ensure the model can handle the input data and output the correct word predictions."
      ],
      "metadata": {
        "id": "hAM1T3dpleNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=LSTMmodel(vocab_size=len(vocab))"
      ],
      "metadata": {
        "id": "ZnNe6GyLU100"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script checks if a GPU is available and sets the device to CUDA for faster training, otherwise defaults to using the CPU, and prints the selected device."
      ],
      "metadata": {
        "id": "prnubYNnloIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBwu4b2TVfBn",
        "outputId": "77c788d8-be92-415a-98fe-bcacf35305a0"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script moves the model to the selected device (GPU or CPU) for training, ensuring the model operates on the appropriate hardware"
      ],
      "metadata": {
        "id": "wz36cHajltW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lItUvTn5V4zX",
        "outputId": "fd99cba0-94e2-488f-c644-7078991335cc"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMmodel(\n",
              "  (emmbedding): Embedding(574, 100)\n",
              "  (lstm): LSTM(100, 150, batch_first=True)\n",
              "  (fc): Linear(in_features=150, out_features=574, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script sets the learning rate to 0.001, which controls the step size during model training to optimize the loss function."
      ],
      "metadata": {
        "id": "H-GnidrtmDmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=50\n",
        "learning_rate=0.001"
      ],
      "metadata": {
        "id": "j0gftzg-WQ6J"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script defines the loss function as CrossEntropyLoss for multi-class classification and initializes the Adam optimizer with the model parameters and a learning rate of 0.00"
      ],
      "metadata": {
        "id": "MQUmjD6TmKlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "PXOyzfu-Wp2X"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script trains the LSTM model for the specified number of epochs, using the data from the DataLoader, calculating the loss, performing backpropagation, and updating the model weights using the Adam optimizer."
      ],
      "metadata": {
        "id": "-6KEVkhumSo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  total_loss=0\n",
        "  for batch_x, batch_y in dataloader:\n",
        "    batch_x, batch_y=batch_x.to(device), batch_y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output=model(batch_x)\n",
        "    loss=criterion(output, batch_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss+=loss.item()\n",
        "  print(f\"Epoch: {epoch+1}, loss {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s-cOE7PmWIu",
        "outputId": "e6f35222-5bd2-46bc-a84c-2fce0f6ace55"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, loss 398.0926\n",
            "Epoch: 2, loss 349.6567\n",
            "Epoch: 3, loss 320.6432\n",
            "Epoch: 4, loss 289.9704\n",
            "Epoch: 5, loss 260.8035\n",
            "Epoch: 6, loss 232.9226\n",
            "Epoch: 7, loss 207.2139\n",
            "Epoch: 8, loss 183.7036\n",
            "Epoch: 9, loss 161.3597\n",
            "Epoch: 10, loss 141.0205\n",
            "Epoch: 11, loss 122.4925\n",
            "Epoch: 12, loss 105.7705\n",
            "Epoch: 13, loss 91.2682\n",
            "Epoch: 14, loss 78.1486\n",
            "Epoch: 15, loss 66.8981\n",
            "Epoch: 16, loss 57.3497\n",
            "Epoch: 17, loss 49.2239\n",
            "Epoch: 18, loss 42.1559\n",
            "Epoch: 19, loss 36.7549\n",
            "Epoch: 20, loss 31.8738\n",
            "Epoch: 21, loss 28.0127\n",
            "Epoch: 22, loss 24.7704\n",
            "Epoch: 23, loss 22.0688\n",
            "Epoch: 24, loss 19.8429\n",
            "Epoch: 25, loss 17.9109\n",
            "Epoch: 26, loss 16.3299\n",
            "Epoch: 27, loss 15.1393\n",
            "Epoch: 28, loss 14.0044\n",
            "Epoch: 29, loss 12.8681\n",
            "Epoch: 30, loss 12.0303\n",
            "Epoch: 31, loss 11.2284\n",
            "Epoch: 32, loss 10.6628\n",
            "Epoch: 33, loss 10.1317\n",
            "Epoch: 34, loss 9.6728\n",
            "Epoch: 35, loss 9.3557\n",
            "Epoch: 36, loss 9.0651\n",
            "Epoch: 37, loss 8.7826\n",
            "Epoch: 38, loss 8.2612\n",
            "Epoch: 39, loss 8.0548\n",
            "Epoch: 40, loss 7.7418\n",
            "Epoch: 41, loss 7.5077\n",
            "Epoch: 42, loss 7.3449\n",
            "Epoch: 43, loss 7.1265\n",
            "Epoch: 44, loss 6.8907\n",
            "Epoch: 45, loss 6.7830\n",
            "Epoch: 46, loss 6.5382\n",
            "Epoch: 47, loss 6.5882\n",
            "Epoch: 48, loss 6.3216\n",
            "Epoch: 49, loss 6.2810\n",
            "Epoch: 50, loss 6.2460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script defines a function for predicting the next word given a text input, by tokenizing, converting to numerical indices, padding the sequence, and using the trained model to generate the next word prediction based on the highest output probability."
      ],
      "metadata": {
        "id": "iHbguHYXmgLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, vocab, text):\n",
        "  tokenized_text=word_tokenize(text.lower())\n",
        "  numerical_text=text_indices(tokenized_text, vocab)\n",
        "  padded_text=torch.tensor([0]*(23-len(numerical_text))+numerical_text, dtype=torch.long).unsqueeze(0).to(device)\n",
        "  output=model(padded_text)\n",
        "  value, index=torch.max(output, dim=1)\n",
        "  # print(list(vocab.keys())[index])\n",
        "  return text +\" \"+ list(vocab.keys())[index]\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "IwatlWnPZLyu"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script predicts the next word after \"Transfer learning is a powerful AI\" by processing the input text through the trained model and returning the predicted next word from the vocabulary."
      ],
      "metadata": {
        "id": "ZaPOXUYdml6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(model, vocab, \"Transfer learning is a powerful AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1rGQAOBgaqzl",
        "outputId": "a64b5c2c-5414-4881-cc52-d277dfc3b704"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Transfer learning is a powerful AI approach'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script generates a sequence of 25 predicted words starting from the input text \"Zero-shot learning\", using the model to predict the next word iteratively and appending it to the input text, with a 0.3-second delay between each prediction."
      ],
      "metadata": {
        "id": "jj_a-cuzmtJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "num_token=30\n",
        "input_text=\"Zero-shot learning\"\n",
        "for token in range(num_token):\n",
        "  output=prediction(model, vocab, input_text)\n",
        "  print(output)\n",
        "  input_text=output\n",
        "  time.sleep(0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owxSrmM5bDkA",
        "outputId": "3b529d45-d753-4458-9905-391a83cbea11"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot learning is\n",
            "Zero-shot learning is a\n",
            "Zero-shot learning is a technique\n",
            "Zero-shot learning is a technique that\n",
            "Zero-shot learning is a technique that allows\n",
            "Zero-shot learning is a technique that allows llms\n",
            "Zero-shot learning is a technique that allows llms to\n",
            "Zero-shot learning is a technique that allows llms to perform\n",
            "Zero-shot learning is a technique that allows llms to perform tasks\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ”\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ” this\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ” this pre-training\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ” this pre-training process\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ” this pre-training process lays\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ” this pre-training process lays the\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ” this pre-training process lays the groundwork\n",
            "Zero-shot learning is a technique that allows llms to perform tasks they haven ’ t explicitly explicitly explicitly pizza ” this pre-training process lays the groundwork for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Km2mhp5DgrOh"
      },
      "execution_count": 150,
      "outputs": []
    }
  ]
}